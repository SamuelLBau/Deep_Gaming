{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#This notebook is meant to run a\n",
    "#It is a slightly modified version of run_training_demo.py\n",
    "#NOTE: There is nothing fancy to display here, I recommend simply running \n",
    "#python run_training_demo.py as the print statements are generally cleaner\n",
    "\n",
    "#USAGE: In cell 2, set the value you want to for env\n",
    "#Kernel must be restart after every iteration. Tensorflow refuses to build the network multiple times for some reason\n",
    "\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "from snake_game import snake_game\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')#Fixes error on DSMP server\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepQ import deepQ\n",
    "\n",
    "\n",
    "#These are used for image pre-processing\n",
    "frame_list = []\n",
    "num_frames = 0\n",
    "\n",
    "def run_training(args):\n",
    "    network= deepQ(**args)\n",
    "\n",
    "    network.train()\n",
    "\n",
    "    del network\n",
    "\n",
    "\n",
    "def empty_preprocess_func(frame):\n",
    "    return np.expand_dims(frame,2)\n",
    "\n",
    "def mspacman_preprocess_func(frame):\n",
    "    mspacman_c = 448 #210 + 164 + 74\n",
    "    img = frame[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_c] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return empty_preprocess_func(img)\n",
    "\n",
    "def carracing_preprocess_func(frame):\n",
    "    global frame_list\n",
    "    global frame_num\n",
    "    img =  frame[:,:,0] * 0.2125\n",
    "    img += frame[:,:,1] * 0.7154\n",
    "    img += frame[:,:,2] * 0.0721\n",
    "    img -= 1\n",
    "    if len(frame_list)==0:\n",
    "        frame_list = np.array([img,img,img,img])\n",
    "    else:\n",
    "        frame_list = np.array([frame_list[1],frame_list[2],frame_list[3],img])\n",
    "    img = frame_list[1] + frame_list[2]*2 + frame_list[3] * 3 + img * 4\n",
    "    img = img/10\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    \n",
    "    return empty_preprocess_func(img)\n",
    "def snake_preprocess_func(frame):\n",
    "    return empty_preprocess_func(frame)\n",
    "def asteroids_preprocess_func(frame):\n",
    "        img = frame[34:210:2, ::2] # crop and downsize\n",
    "        img = img.sum(axis=2) # to greyscale\n",
    "        return empty_preprocess_func(img)\n",
    "\n",
    "\n",
    "\n",
    "def configure_mspacman_training():\n",
    "    args = {}\n",
    "\n",
    "\n",
    "    args[\"game_type\"] = \"MsPacman-v0\"\n",
    "    args[\"env\"] = gym.make(args[\"game_type\"])\n",
    "    args[\"proto\"] = \"cfn/MsPacman-v0.prototxt\"\n",
    "    args[\"action_space\"] = list(range(9))\n",
    "    args[\"preprocess_func\"] = mspacman_preprocess_func\n",
    "    args[\"n_steps\"]=4000000\n",
    "    args[\"momentum\"]=.95\n",
    "    args[\"learning_rate\"] = .001\n",
    "    args[\"discount\"] = .99\n",
    "    args[\"epsilon_min\"]=.100000\n",
    "    args[\"epsilon_max\"]=1.000000\n",
    "    \n",
    "    \n",
    "    args[\"game_skip\"] = 80\n",
    "    args[\"minibatch_size\"] = 32\n",
    "    args[\"fresh\"] = False\n",
    "    args[\"learning_interval\"] = 4\n",
    "\n",
    "    args[\"save_rewards\"] = True\n",
    "    args[\"jupyter_prints\"]  = True\n",
    "\n",
    "    return args\n",
    "    \n",
    "def configure_carracing_training():\n",
    "    args = {} \n",
    "    args[\"game_type\"] = \"CarRacing-v0\"\n",
    "    args[\"env\"] = gym.make(args[\"game_type\"])\n",
    "    args[\"proto\"] = \"cfn/CarRacing-v0.prototxt\"\n",
    "    args[\"preprocess_func\"] = carracing_preprocess_func\n",
    "    \n",
    "    #Discretize the action space\n",
    "    render = True\n",
    "    range0 = [-1,0,1]\n",
    "    range1 = [1,0]\n",
    "    range2 = [.2,0]\n",
    "    action_space = []\n",
    "    for i in range0:\n",
    "        for j in range1:\n",
    "            for k in range2:\n",
    "                action_space.append([i,j,k])\n",
    "    args[\"action_space\"] = action_space\n",
    "    \n",
    "    args[\"n_steps\"]=4000000\n",
    "    args[\"game_skip\"] = 50\n",
    "    args[\"minibatch_size\"] = 50\n",
    "    args[\"discount\"] = .95\n",
    "    args[\"fresh\"] = False\n",
    "    args[\"save_rewards\"] = True\n",
    "    args[\"learning_interval\"] = 1\n",
    "    args[\"max_neg_reward_steps\"] = 150\n",
    "    args[\"jupyter_prints\"]  = True\n",
    "    return args\n",
    "\n",
    "def configure_snake_training():\n",
    "    args = {} \n",
    "    args[\"game_type\"] = \"snake\"\n",
    "    args[\"env\"] = snake_game(board_size=[25,25])\n",
    "    args[\"proto\"] = \"cfn/snakenet.prototxt\"\n",
    "    args[\"preprocess_func\"] = snake_preprocess_func\n",
    "    args[\"action_space\"] = list(range(4))\n",
    "\n",
    "    args[\"save_rewards\"] = True\n",
    "    args[\"game_skip\"] = 0\n",
    "    args[\"minibatch_size\"] = 30\n",
    "    args[\"learning_interval\"] = 1\n",
    "    args[\"jupyter_prints\"]  = True\n",
    "\n",
    "    return args\n",
    "\n",
    "def configure_asteroids_training():\n",
    "    args = {}\n",
    "\n",
    "\n",
    "    args[\"game_type\"] = \"Asteroids-v0\"\n",
    "    args[\"env\"] = gym.make(args[\"game_type\"])\n",
    "    args[\"proto\"] = \"cfn/Asteroids.prototxt\"\n",
    "    args[\"action_space\"] = list(range(14))\n",
    "    args[\"preprocess_func\"] = asteroids_preprocess_func\n",
    "    args[\"n_steps\"]=4000000\n",
    "    args[\"momentum\"]=.95\n",
    "    args[\"learning_rate\"] = .001\n",
    "    args[\"discount\"] = .99\n",
    "    args[\"epsilon_min\"]=.100000\n",
    "    args[\"epsilon_max\"]=1.000000\n",
    "    \n",
    "    \n",
    "    args[\"game_skip\"] = 0\n",
    "    args[\"minibatch_size\"] = 32\n",
    "    args[\"fresh\"] = False\n",
    "    args[\"learning_interval\"] = 1\n",
    "\n",
    "    args[\"save_rewards\"] = True\n",
    "    args[\"jupyter_prints\"]  = True\n",
    "\n",
    "    return args   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MsPacman training\n",
      "Building Convolutional layer with following parameters: num_filters 32, kernel_size [8, 8], padding same, stride [4, 4], use_bias False\n",
      "Building Convolutional layer with following parameters: num_filters 64, kernel_size [4, 4], padding same, stride [2, 2], use_bias False\n",
      "Building Convolutional layer with following parameters: num_filters 64, kernel_size [3, 3], padding same, stride [1, 1], use_bias False\n",
      "Building dense layer with num_output 512, initializer <tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C1A28D3470> and activation <function relu at 0x000001C1AF1D0C80>\n",
      "Building dense layer with num_output 9, initializer <tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C1B26C6F60> and activation None\n",
      "Building Convolutional layer with following parameters: num_filters 32, kernel_size [8, 8], padding same, stride [4, 4], use_bias False\n",
      "Building Convolutional layer with following parameters: num_filters 64, kernel_size [4, 4], padding same, stride [2, 2], use_bias False\n",
      "Building Convolutional layer with following parameters: num_filters 64, kernel_size [3, 3], padding same, stride [1, 1], use_bias False\n",
      "Building dense layer with num_output 512, initializer <tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C1B2749EB8> and activation <function relu at 0x000001C1AF1D0C80>\n",
      "Building dense layer with num_output 9, initializer <tensorflow.python.ops.init_ops.VarianceScaling object at 0x000001C1B274A160> and activation None\n",
      "/conv2d/bias:0 <tf.Variable 'q_networks/online/conv2d/bias:0' shape=(32,) dtype=float32_ref>\n",
      "/conv2d_2/bias:0 <tf.Variable 'q_networks/online/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "/conv2d_1/bias:0 <tf.Variable 'q_networks/online/conv2d_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "/conv2d_1/kernel:0 <tf.Variable 'q_networks/online/conv2d_1/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>\n",
      "/conv2d/kernel:0 <tf.Variable 'q_networks/online/conv2d/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>\n",
      "/conv2d_2/kernel:0 <tf.Variable 'q_networks/online/conv2d_2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "/dense/kernel:0 <tf.Variable 'q_networks/online/dense/kernel:0' shape=(7040, 512) dtype=float32_ref>\n",
      "/dense_1/kernel:0 <tf.Variable 'q_networks/online/dense_1/kernel:0' shape=(512, 9) dtype=float32_ref>\n",
      "/dense/bias:0 <tf.Variable 'q_networks/online/dense/bias:0' shape=(512,) dtype=float32_ref>\n",
      "/dense_1/bias:0 <tf.Variable 'q_networks/online/dense_1/bias:0' shape=(9,) dtype=float32_ref>\n",
      "DONE\n",
      "CNN built succesfully\n",
      "Input shape = (?, 88, 80, 1)\n",
      "Output shape = (?, 9)\n",
      "0.1 <class 'float'>\n",
      "MsPacman-v0_PacNet: Step  1427906 of  4000000 (35.6977%),\tAverage Reward 530.000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b3a174c2eb8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running MsPacman training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigure_mspacman_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"CarRacing-v0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running CarRacing training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-80340416b5b3>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdeepQ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Files\\school_BU\\UCSDGradSchool\\Deep_Gaming\\src\\deepQ.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             GARBAGE, loss = self.session.run([self.training_op, self.loss], feed_dict={\n\u001b[1;32m--> 366\u001b[1;33m                 self.online_input: sampled_state_vals, self.action: sampled_action_vals, self.sampled_vals: sampled_vals})\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;31m# Regularly copy the online DQN to the target DQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samuel\\appdata\\local\\conda\\conda\\envs\\deep_gaming\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samuel\\appdata\\local\\conda\\conda\\envs\\deep_gaming\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samuel\\appdata\\local\\conda\\conda\\envs\\deep_gaming\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samuel\\appdata\\local\\conda\\conda\\envs\\deep_gaming\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samuel\\appdata\\local\\conda\\conda\\envs\\deep_gaming\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samuel\\appdata\\local\\conda\\conda\\envs\\deep_gaming\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = \"MsPacman-v0\" #Supported envs \"MsPacman-v0\" \"Asteroids-v0\" \"CarRacing-v0\" \"snake\"\n",
    "\n",
    "if env == \"MsPacman-v0\":\n",
    "    print(\"Running MsPacman training\")\n",
    "    args = configure_mspacman_training()\n",
    "    run_training(args)\n",
    "elif env == \"CarRacing-v0\":\n",
    "    print(\"Running CarRacing training\")\n",
    "    try:\n",
    "        args = configure_carracing_training()\n",
    "        run_training(args)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to run CarRacing example,\",str(e))\n",
    "        print(\"CarRacing requires the Box2D Library, which can by tricky to install\")\n",
    "elif env.lower() == \"snake\":\n",
    "    print(\"Running Snake training\")\n",
    "    #try:\n",
    "    args = configure_snake_training()\n",
    "    run_training(args)\n",
    "    #except Exception as e:\n",
    "    #print(\"Failed to run Snake example,\",str(e))\n",
    "elif env == \"Asteroids-v0\":\n",
    "    print(\"Running Asteroids training\")\n",
    "    args = configure_asteroids_training()\n",
    "    run_training(args)\n",
    "else:\n",
    "    print(\"Unsupported training environment %s\"%(env))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Gaming",
   "language": "python",
   "name": "deep_gaming"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
