\documentclass[11pt]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.25cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{cases}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Deep Gaming}
\author{Samuel Bauza A10025697
\and
Bryan Kerr A10128123
\and
Edward Zamora A53239772}

\date{June 15, 2018}

\begin{document}
\maketitle

\section{Introduction}

This project explores the viability of a using Deep Learning to learn to play a collection of simple computer games. In particular, we will use a method of merging Deep Learning and Reinforcement Learning called Deep Q Learning. We create a network and use it to play a number of computer games using only the pixel images as input. Ultimately we successfully learn a number of games, but find that some games are not well suited to Convolutional Neural Networks as a basis for learning.

\section{Deep Learning Framework Exploration}

Before working on building various networks to play games, we examined different CNN frameworks to build upon. Keras and Tensorflow both have implementations in python. Our team has slightly more experience with Tensorflow, and it comes pre-installed on the DSMP server. Because of these reasons, we decided to build our framework using Tensorflow.

\subsection{Keras}

BRYAN: I believe this was your section.

\subsection{Caffe}

EDWARD: I believe this was your section

\subsection{Tensorflow}

Tensorflow is an open source machine learning framework that is capable of utilizing a system's GPU to accelerate learning. It allows for very strong control over the various parts of the network. This level of control comes at the cost of being quite a bit more difficult to use. 

In general, layers are built one at a time until the entire architecture has been specified. Data can then be passed as input and the entire architecture processed in a single step. Additionally, learning rates, parameter types and structure are all fully specifiable.

We decided to use Tensorflow over Keras due to this additional control. This enables to test various small tweaks to the network to study their effect and relative ability to learn.

\section{Code Framework}

This section will discuss the various components and tools used by our program. This section is not meant to explain the usage of the tool, as it will be described in the README.md file.

\subsection{Deep Q Learning}

Deep Q Learning is a method that has been used to apply deep learning to reinforcement learning challenges. Q Learning is an approach to reinforcement learning in which the algorithm develops a 'Q' table, specifying the expected total reward when taking a particular action. This table is approximated for each possible state in the environment.

TODO: FINISH THIS SEGMENT
Deep Q Learning takes this process a step further, by using Deep Learning techniques to approximate the Q table

TODO: DISCUSS CNNS

TODO: DISCUSS LOSS FUNCTION

TODO: DISCUSS REINFORCEMENT LEARNING

\subsection{Network Generator}

As mentioned before, Tensorflow is used to implement the Convolutional Neural Network. We were initially interested in loading a variety of existing neural networks. Because many of these networks have caffe specifications, we implemented a .prototxt to Tensorflow network parser. 

We later discovered, that the Caffe and Tensorflow Frameworks do not fully overlap in their capabilities. Because of this, we adapted the format to better fit Tensorflow layer generation. Though this means it will no longer be fully compatible with existing networks, it does give us an easy means to update and test different networks with various games.

Now, the network is assembled layer by layer in a .prototxt configuration file. Here, the layer type, parameters, and activation functions are all specified in this file. This allows us to build various architectures and interchange them using simple command-line arguments.

\subsection{Training Framework}

The learning process can be repeated indefinitely, but here is repeated only for a pre-set number of learning steps. The current state is input to the online network, and the output values are used to select an optimal action. There will always be some probability that a random action is taken. This helps ensure that the system does not get stuck repeating the same actions. This probability begins high, but decreases as the number of training iterations increases, meaning the system will initially tend to explore the space, while eventually beginning to trust its own estimates.

The environment's state is updated using the selected action, producing the new state, a reward for that action, and whether or not the current game has terminated. This set of information is loaded into memory and stored for later reference.

A batch of previous state are pulled from memory and passed through the target network, and the same values mentioned above are collected from each. The states and actions taken are passed into the loss function. The rewards are added to a weighted amount of the expected final reward for the optimal action. This is the y value passed into the loss function discussed in the next section. 

The loss function is then calculated and used to update the online network parameters. Occasionally, the target network is updated to match the current online values. This allows the target network to improve in tandem with the online network, keeping its loss estimates accurate. At a regular interval, the weights of the network are saved, allowing the learning to be paused and the current performance examined.

\subsection{Loss function}

Due to time constraints and concerns about limiting our test space, we selected and implemented a single loss function. This loss function will be common to all environments and networks we test. This loss is minimized at each learning step, and used to power the back-propagation to update the online network values.

The error is the different between the optimal reward expected by the target network, and the reward from the online network when taking the selected action.

$loss = mean(error^2)$ if $error\in(0.0,1.0)$

$loss = (2*error) - 1$ if $error\in(1.0,\infty)$

TODO: ADD GRAPH OF THIS EQUATION

\subsection{OpenAI Gym}

OpenAI Gym is a toolkit that provides various environments in which to develop and compare reinforcement learning algorithms. These environments range from simple physics simulators, such as inverted pendulum balancing, to old Atari-style games, to advanced robotic simulation environments. These environments provide information about the current state, and simulate the results of various actions upon the environment.

Our usage will largely focus on the subsets of gym environments under the Atari collection. These are emulations of old, recognizable games, such as MsPacman, Space Invaders, and Asteroids. These will be the environments in which our architecture gains experience.

\section{Explored Games}

\subsection{Snake}

SAM:

\subsection{MsPacman}

BRYAN OR SAM: 

\subsection{Space Invaders}

\subsection{Car Racing}

EDWARD:

\section{Results}

\subsection{Snake}

SAM

\subsection{MsPacman}

BRYAN OR SAM: 

\subsection{Space Invaders}

\subsection{Car Racing}

EDWARD:

\section{END OF OUR SECTION. DELETE THIS BEFORE SUBMISSION}

\section{Some examples to get started}

\subsection{How to include Figures}

First you have to upload the image file from your computer using the upload link the project menu. Then use the includegraphics command to include it in your document. Use the figure environment and the caption command to add a number and a caption to your figure. See the code for Figure \ref{fig:frog} in this section for an example.

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{frog.jpg}
\caption{\label{fig:frog}This frog was uploaded via the project menu.}
\end{figure}

\subsection{How to add Comments}

Comments can be added to your project by clicking on the comment icon in the toolbar above. % * <john.hammersley@gmail.com> 2016-07-03T09:54:16.211Z:
%
% Here's an example comment!
%
To reply to a comment, simply click the reply button in the lower right corner of the comment, and you can close them when you're done.

Comments can also be added to the margins of the compiled PDF using the todo command\todo{Here's a comment in the margin!}, as shown in the example on the right. You can also add inline comments:

\todo[inline, color=green!40]{This is an inline comment.}

\subsection{How to add Tables}

Use the table and tabular commands for basic tables --- see Table~\ref{tab:widgets}, for example. 

\begin{table}[h]
\centering
\begin{tabular}{l|r}
Item & Quantity \\\hline
Widgets & 42 \\
Gadgets & 13
\end{tabular}
\caption{\label{tab:widgets}An example table.}
\end{table}

\subsection{How to write Mathematics}

\LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
\[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i\]
denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


\subsection{How to create Sections and Subsections}

Use section and subsections to organize your document. Simply use the section and subsection buttons in the toolbar to create them, and we'll handle all the formatting and numbering automatically.

\subsection{How to add Lists}

You can make lists with automatic numbering \dots

\begin{enumerate}
\item Like this,
\item and like this.
\end{enumerate}
\dots or bullet points \dots
\begin{itemize}
\item Like this,
\item and like this.
\end{itemize}

\subsection{How to add Citations and a References List}

You can upload a \verb|.bib| file containing your BibTeX entries, created with JabRef; or import your \href{https://www.overleaf.com/blog/184}{Mendeley}, CiteULike or Zotero library as a \verb|.bib| file. You can then cite entries from it, like this: \cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|.

You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

We hope you find Overleaf useful, and please let us know if you have any feedback using the help menu above --- or use the contact form at \url{https://www.overleaf.com/contact}!

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}